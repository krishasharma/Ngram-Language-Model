# N-gram Language Modeling for CSE 143

This repository contains my implementation of N-gram language models as part of Assignment 2 for CSE 143: Intro to Natural Language Processing. The assignment involves implementing and evaluating unigram, bigram, and trigram models with MLE, additive smoothing, and linear interpolation smoothing. The models are tested on subsets of the One Billion Word Language Modeling Benchmark.

## Table of Contents
- [Overview](#overview)
- [Installation](#installation)
- [Usage](#usage)
- [Programming Tasks](#programming-tasks)
  - [N-gram Language Modeling](#n-gram-language-modeling)
  - [Additive Smoothing](#additive-smoothing)
  - [Linear Interpolation Smoothing](#linear-interpolation-smoothing)
- [Data](#data)
- [Deliverables](#deliverables)

## Overview
This project focuses on building N-gram language models for natural language processing tasks. The models are used to compute perplexity on training, development, and test datasets. I also implemented various smoothing techniques to improve model performance, such as additive smoothing and linear interpolation smoothing.

## Installation
To set up and run the project, you will need Python 3.